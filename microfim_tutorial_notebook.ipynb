{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "microfim_tutorial_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wugJz24DOCwc"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AaEWhzPO3j2"
      },
      "source": [
        "Materials avalaible in the repository:\n",
        "\n",
        "*   files for tests and tutorials (dir: **tutorials**)\n",
        "*   functions (dir: **functions**)\n",
        "*   template files to be filled (dir: **template_inputs**)\n",
        "*   guided scripts (**script_***)\n",
        "*   requirements.txt (for conda installation)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee6isMtEOdEx"
      },
      "source": [
        "Prerequisites:\n",
        "\n",
        "*   Python > 3 (https://www.python.org/)\n",
        "*   Conda or Miniconda (https://conda.io/projects/conda/en/latest/user-guide/install/index.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iwh-TgBWFyjP"
      },
      "source": [
        "# cloning repository via git (or download zip folder drictly from the github page)\n",
        "\n",
        "git clone https://github.com/qLSLab/microFim.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq0KNL4HODxG"
      },
      "source": [
        "# create conda env\n",
        "\n",
        "conda create --name microFIM --file requirements.txt --channel default --channel conda-forge --channel plotly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFCTei9VOEgX"
      },
      "source": [
        "# Script usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KYZdOwqPmy-"
      },
      "source": [
        "Guided scripts must be run in the main directory (within microFIM, after cloning the repository and create the environment). The scripts are 'interactive', with auto-completion for an easy usage.\n",
        "\n",
        "We suggest to create a specific directory for your project, in order to set it for inputs and outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw3X7Ir3ORTs"
      },
      "source": [
        "python script_1_filtertable.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ebx4zXH0zYcl"
      },
      "source": [
        "This script can be used to filter your otu/taxa table based on a list of samples.\n",
        "Files required and mandatory instructions:\n",
        "* otu/esv/taxa table - the column name of OTU or TAXA must be '#ID'\n",
        "* sample list  - the first row of your sample list must be '#SampleID'\n",
        "\n",
        "The script will ask you to set the input directory and the two files mentioned -\n",
        "otu/esv/taxa table and sample list. The format of the file does not matter at this stage,\n",
        "the script will ask you the type of separator.\n",
        "\n",
        "The output file will be a filtered CSV file saved into the input directory\n",
        "(in order to allow subsequent analysis)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZhbfPmFkdHM"
      },
      "source": [
        "python script_2_tableconversion.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKDDHa37zZB_"
      },
      "source": [
        "This script can be used to convert a otu/esv/taxa table into a list of transactions.\n",
        "At this stage, do not worry about the format of the input. The script will ask\n",
        "you which is the separator.\n",
        "\n",
        "The output will be saved as a list of transactions into input directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9LGdXqGkcxL"
      },
      "source": [
        "python script_3_microfimcalculation.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdmH97GYzaQv"
      },
      "source": [
        "This script calculate microbial patterns!\n",
        "Files:\n",
        "- otu/esv/taxa table previously converted in transactions\n",
        "- file with parameters in .csv format (support, zmin and zmax + type of report)\n",
        "    template available in the tutorial folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT_shLFpkcze"
      },
      "source": [
        "script_4_additionalmeasures.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2GOxGHvzZtK"
      },
      "source": [
        "This script calculate additional interest measures that can be used\n",
        "to filter results. Currently, all-confidence metric is available (see README for details)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qwk7rLz8kc17"
      },
      "source": [
        "script_5_generatepatterntable.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnxfeKoSvYgj"
      },
      "source": [
        "This script can be used to create the pattern table.\n",
        "Inputs:\n",
        "- pattern results;\n",
        "- metadata file;\n",
        "- transactional file.\n",
        "\n",
        "The output will be saved as a CSV dataframe (with and without\n",
        "inrerest measures) into input directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK6IIV87zT92"
      },
      "source": [
        "# # available from monday 15\n",
        "\n",
        "script_6_generateplots.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pbypp4gOJze"
      },
      "source": [
        "# Library usage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3urtkQtPnQ-"
      },
      "source": [
        "microFIM python functions were divided into thematic sections, in order to promote the integration of new functions and an easy development of the tool. Here we present three scripts that can be used on test/test1.csv files and the metadata and parameters related. \n",
        " \n",
        "\n",
        "*   The first one (named microFIM_example_code_1.py) filter the data table and convert it in transactional file. To filter, use a metadata files removing lines of samples you want to exclude.\n",
        "*   The second create calculate patterns and create the pattern table with and without interest measures.\n",
        "*   The third create visualizations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC_K5av_OQ7k"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "from csv import writer\n",
        "import readline\n",
        "import re\n",
        "import string\n",
        "\n",
        "import fim\n",
        "import functions.microdir as md\n",
        "import functions.microfim as mf\n",
        "import functions.microimport as mi\n",
        "import functions.microinterestmeasures as mim\n",
        "\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn import manifold\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" microFIM example code on test/test1.csv files\n",
        "of microFIM github repository\n",
        "Input files to run microFIM:\n",
        "- test1.csv\n",
        "- metadata_test1.csv\n",
        "- parameters_test1.csv\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# set dir\n",
        "set_dir = 'test'\n",
        "data_dir = md.set_inputs_dir(set_dir)\n",
        "print(data_dir)\n",
        "\n",
        "# change dir\n",
        "os.chdir(data_dir)\n",
        "\n",
        "# import files\n",
        "\n",
        "metadata = pd.read_csv(os.path.join(data_dir, 'metadata_test1.csv'), header=0, index_col=None)\n",
        "print(metadata)\n",
        "data_table_name = 'test1.csv'\n",
        "data_table = pd.read_csv(os.path.join(data_dir, 'test1.csv'), header=0, index_col=None, engine='python')\n",
        "print(data_table)\n",
        "#parameters = pd.read_csv(os.path.join(data_dir, data_table), sep=sep, header=0, index_col=None, engine='python')\n",
        "\n",
        "\n",
        "# FILTER DATA TABLE VIA SAMPLE METADATA\n",
        "# convert sample_list into a list\n",
        "samples = metadata['#SampleID'].to_list()\n",
        "\n",
        "# extract '#ID' column (otu/taxa) - see Docoumentation for details\n",
        "id = data_table[['#ID']]\n",
        "samples_table = data_table[[*samples]]\n",
        "\n",
        "# concat datasets\n",
        "new_data = pd.concat([id, samples_table], axis=1)\n",
        "#print(new_data.info())\n",
        "\n",
        "\n",
        "# remove rows with zeros\n",
        "no_zeros = (new_data.iloc[:,1:] != 0).any(axis=1)\n",
        "new_data = new_data.loc[no_zeros]\n",
        "print(new_data)\n",
        "\n",
        "\n",
        "# CONVERT IN TRANSACTIONAL File\n",
        "\n",
        "file_name = data_table_name.split('.')\n",
        "print(file_name)\n",
        "\n",
        "# remove space from ID column\n",
        "new_data['#ID'] = new_data['#ID'].str.replace(' ','_')\n",
        "\n",
        "print(new_data)\n",
        "\n",
        "n_cols = new_data.shape[1] - 1\n",
        "#print(n_cols)\n",
        "n_rows = new_data.shape[0] - 1\n",
        "#print(n_rows)\n",
        "\n",
        "\n",
        "t_list = mf.write_transactions(n_cols, n_rows, new_data)\n",
        "#print(t_list)\n",
        "\n",
        "# save as transaction list\n",
        "with open(data_dir + '/' + 'transactions_' + file_name[0], 'w') as f:\n",
        "    wr = csv.writer(f)\n",
        "    wr.writerows(t_list)\n",
        "\n",
        "# convert commas in spaces (for the next steps)\n",
        "# remove old output to clean folder\n",
        "output = 'transactions_' + file_name[0]\n",
        "\n",
        "print(f'\\n\\n> File converted and saved as ' + output + '.csv' + ' in ' + data_dir + '\\n\\n')\n",
        "\n",
        "# this last script must be run in bash. If you use a Linux terminal, rm command\n",
        "# will not be necessary\n",
        "print(f'\\n\\n> Now run from your command line in {data_dir}:\\n\\n \\\n",
        "sed -i -e \"s/,/ /g\" {output}\\n\\n \\\n",
        "rm {output}-e\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNrLw3ga0Idu"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "from csv import writer\n",
        "import readline\n",
        "import re\n",
        "import string\n",
        "\n",
        "import fim\n",
        "import functions.microdir as md\n",
        "import functions.microfim as mf\n",
        "import functions.microimport as mi\n",
        "import functions.microinterestmeasures as mim\n",
        "\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn import manifold\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" microFIM example code on test/test1.csv files\n",
        "of microFIM github repository\n",
        "Input files to run microFIM:\n",
        "- test1.csv\n",
        "- transactional file (can be obtained with microFIM_example_code_1.py)\n",
        "- metadata_test1.csv\n",
        "- parameters_test1.csv\n",
        "\n",
        "Default is itemsets patterns, but also closed and maximal can be calculated.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# set dir\n",
        "set_dir = 'test'\n",
        "data_dir = md.set_inputs_dir(set_dir)\n",
        "print(data_dir)\n",
        "\n",
        "# change dir\n",
        "os.chdir(data_dir)\n",
        "\n",
        "# import files\n",
        "\n",
        "metadata = pd.read_csv(os.path.join(data_dir, 'metadata_test1.csv'), header=0, index_col=None)\n",
        "print(metadata)\n",
        "data_table_name = 'test1.csv'\n",
        "data_table = pd.read_csv(os.path.join(data_dir, 'test1.csv'), header=0, index_col=None, engine='python')\n",
        "print(data_table)\n",
        "par_file = 'parameters_test1.csv'\n",
        "trans_file = 'transactions_test1'\n",
        "\n",
        "# import transactions and file with paramaters\n",
        "t = mf.read_transaction(os.path.join(data_dir, trans_file))\n",
        "print(t)\n",
        "\n",
        "minsupp, zmin, zmax= mi.itemsets_parameters(data_dir, par_file)\n",
        "print(minsupp)\n",
        "print(zmin)\n",
        "print(zmax)\n",
        "\n",
        "#sys.exit()\n",
        "\n",
        "# set fim options\n",
        "report= '[asS' # mandatory\n",
        "to_calculate = 'i' # default (can be changed in c or m)\n",
        "\n",
        "\n",
        "# run eclat (mandatory)\n",
        "if to_calculate == 'i':\n",
        "    results = fim.eclat(t, target='s', supp=minsupp, zmin=zmin, report=report)\n",
        "elif to_calculate == 'c':\n",
        "    results = fim.eclat(t, target='c', supp=minsupp, zmin=zmin, report=report)\n",
        "elif to_calculate == 'm':\n",
        "    results = fim.eclat(t, target='m', supp=minsupp, zmin=zmin, report=report)\n",
        "\n",
        "print(results)\n",
        "# define output name\n",
        "output_file = 'patterns_test1'\n",
        "\n",
        "# write results\n",
        "file = open(data_dir + '/' + output_file + '.csv', 'w+', newline ='')\n",
        "# writing the data into the file\n",
        "with file:\n",
        "    write = csv.writer(file)\n",
        "    write.writerows(results)\n",
        "\n",
        "\n",
        "out_file = data_dir + '/' + output_file + '.csv'\n",
        "new_out_file = data_dir + 'df_' + output_file + '.csv'\n",
        "with open(out_file, 'r') as f, open(new_out_file, 'w') as fo:\n",
        "    for line in f:\n",
        "        fo.write(line.replace('\"', '').replace(\"'\", \"\").replace('),[', ')/[').replace(')', '').replace('(', '').replace('[', '').replace(']', ''))\n",
        "\n",
        "\n",
        "## convert itemsets results into a dataframe\n",
        "df = mf.itemsets_dataframe(new_out_file)\n",
        "df.to_csv(new_out_file, index=False)\n",
        "\n",
        "print(df)\n",
        "\n",
        "print(out_file)\n",
        "os.remove(out_file)\n",
        "\n",
        "print('Results saved as ' + new_out_file + ' in ' + data_dir + '\\n\\n')\n",
        "\n",
        "#sys.exit()\n",
        "\n",
        "# CALCULATE ADDITIONAL METRICS\n",
        "# calculate occurrences for each id\n",
        "frequency = mim.calculate_ids_occurrence(data_table)\n",
        "print(frequency)\n",
        "\n",
        "# calculate len of trans_file\n",
        "lines_in_file = open(os.path.join(data_dir, trans_file), 'r').readlines()\n",
        "#print(lines_in_file)\n",
        "number_of_lines = float(len(lines_in_file))\n",
        "\n",
        "#print(number_of_lines)\n",
        "\n",
        "\n",
        "data_allc_update = mim.all_confidence(df, frequency, number_of_lines)\n",
        "#print(data_allc_update)\n",
        "\n",
        "\n",
        "# write file\n",
        "file_name = 'addm_patterns_test1'\n",
        "\n",
        "data_allc_update.to_csv(os.path.join(data_dir, 'df_' + file_name + '.csv'), index=False)\n",
        "\n",
        "\n",
        "print('Results saved as df_' + file_name + '.csv in ' + data_dir + '\\n\\n')\n",
        "\n",
        "\n",
        "## GENERATE PATTERN TABLE\n",
        "\n",
        "col_patterns = mf.set_patterns_for_matching(data_allc_update)\n",
        "transactional_list = mf.set_transdata_for_matching(data_dir, trans_file)\n",
        "meta_file = 'metadata_test1.csv'\n",
        "sep = ','\n",
        "pattern_table = mf.generate_pattern_occurrences(data_dir, data_allc_update, transactional_list, meta_file, sep)\n",
        "\n",
        "df_pattern_table = mf.concat_tables(df, pattern_table)\n",
        "print(df_pattern_table)\n",
        "\n",
        "# only 0 and 1\n",
        "df_pattern_table_clean = df_pattern_table.drop(['Samples', 'Support', 'Support(%)', 'Pattern length', 'All-confidence'], axis=1)\n",
        "\n",
        "#sys.exit()\n",
        "\n",
        "# save\n",
        "output_file = 'pattern_table_test'\n",
        "\n",
        "df_pattern_table.to_csv(os.path.join(data_dir, output_file + '_complete.csv'), index=False)\n",
        "df_pattern_table_clean.to_csv(os.path.join(data_dir, output_file + '.csv'), index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_Q94idB0Is-"
      },
      "source": [
        "# available from monday 15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZei-UgSWlXu"
      },
      "source": [
        "# Integration in QIIME2 framework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-R0CIPvduij"
      },
      "source": [
        "## Export taxa tables for microFIM analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m3DZ4fNWnRB"
      },
      "source": [
        "# activate the env (if you do not installed QIIME2 yet, please see https://docs.qiime2.org/2021.8/getting-started/)\n",
        "\n",
        "conda activate qiime2-2020.8 # example version\n",
        "\n",
        "\n",
        "# export biom file form qza\n",
        "\n",
        "qiime tools export --input-path table.qza --output-path exported-feature-table\n",
        "\n",
        "\n",
        "# convert biom file to tsv\n",
        "\n",
        "biom convert -i exported-feature-table/feature-table.biom -o feature-table.tsv --to-tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0szuTJsGAp1"
      },
      "source": [
        "# substitue #OTU ID with #ID\n",
        "\n",
        "sed -i -e \"s/#OTU ID/#ID/g\" feature-table.tsv\n",
        "\n",
        "\n",
        "# remove first row\n",
        "\n",
        "sed -i '1d' feature-table.tsv\n",
        "\n",
        "\n",
        "## READY TO BE IMPORTED IN microFIM ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0efzf4ddw7R"
      },
      "source": [
        "## Import pattern tables in qza format to perform QIIME2 analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU0CpEFCUuK7"
      },
      "source": [
        "Change 'Pattern' column in #OTU ID before converting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDQ0LV7ydy5U"
      },
      "source": [
        "# convert in biom file\n",
        "\n",
        "biom convert -i pattern_table_test.tsv \\\n",
        "  -o pattern_table_test.biom --table-type=\"OTU table\" --to-json\n",
        "\n",
        "\n",
        "# import in qiime2\n",
        "\n",
        "qiime tools import \\\n",
        "  --input-path pattern_table_test.biom \\\n",
        "  --type 'FeatureTable[Frequency]' \\\n",
        "  --input-format BIOMV100Format \\\n",
        "  --output-path pattern_table_test.qza"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}